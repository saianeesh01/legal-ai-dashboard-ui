Below is a **copy-and-paste “one-shot” prompt** you can feed to your local model ( *Mistral-7B-Instruct*, *Llama-3-8B-Instruct*, *Phi-3-mini* …) **right after OCR / chunk retrieval**.
It folds three tasks into a *single* request, so you burn **one model call per document**, and later you can send much cheaper, narrow “follow-up” questions that reuse the same `context` array.

````text
SYSTEM  
You are LegalDoc AI, a concise, citation-aware assistant.  
All answers **must be derived only from the provided context**.  
Never invent facts, statutes, or page numbers.  
When unsure, reply “I don’t know from the context.”

#

USER  
<<DOC_META>>  
- file_name: {{file_name}}  
- job_id: {{job_id}}  
- total_pages: {{page_count}}  

<<CONTEXT>>  
{{context_chunks}}   # array of page-level text snippets, each max 400 chars,
                     # already ranked by vector similarity to “proposal” seed words

<<TASKS>>  
1. **Classify** — Is this a *proposal* document?  
   • Output exactly `"proposal"` or `"non-proposal"` plus a 0-1 confidence score.  
2. **Executive summary** — ≤ 120 words, bullet style, plain English.  
3. **Suggestions** — up to 5 numbered fixes that would strengthen the document.  
4. **Critical deadlines** — return as JSON list:  
   ```json
   {
     "deadlines": [
       { "description": "Renewal notice", "date": "2024-11-02", "page": 3 },
       …
     ]
   }
````

Respond in the following *single* JSON block **and nothing else**:

```json
{
  "verdict": "<proposal | non-proposal>",
  "confidence": 0.<2 decimals>,
  "summary": "<bullet summary>",
  "suggestions": ["<text>", …],
  "deadlines": [ { "description": "...", "date": "YYYY-MM-DD", "page": N }, …]
}
```

````

### How to use it without blowing your Replit quota

| Step | What you send to the model | Why it’s cheap |
|------|----------------------------|----------------|
| **Upload phase** | The prompt above with **only the top ~10 context chunks** (≈3 KB) | Single call → classification + summary + deadlines |
| **Store JSON**  | Save the model’s JSON to `results/<job_id>.json` | Later UI reads it—no new LLM call |
| **Ask question** | On each user query send a tiny prompt:  ```SYSTEM same rules  USER Q: "{{question}}"  CONTEXT: {{top_chunks}} ``` | Only the question + 3–5 chunks → ~1 KB per call |

### Tips that keep it snappy on free Replit

* **Pre-filter** with a keyword regex (`proposal|request for proposal|RFP`) to skip the LLM if obviously non-proposal.  
* **Compress context**: strip whitespace & keep the first 400 chars of each chunk.  
* Use **`mistral-7b Q4_K_M` quant** → <3 GB RAM, runs <1 sec per 1 K tokens on Replit’s 0.3 vCPU burst.  
* Update `/api/status` every page so the front-end bar moves even while PaddleOCR is still scanning.

Copy the block, drop it into your Flask worker right after OCR, and you’ll get the verdict, summary, suggestions, and deadlines in one shot—no extra “agent” loops or credit burn.
::contentReference[oaicite:0]{index=0}
````
