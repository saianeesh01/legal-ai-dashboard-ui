services:
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: legal-ai-frontend
    ports:
      - "5000:5000"
    environment:
      DATABASE_URL: "postgresql://neondb_owner:npg_oNhiQsanu4p2@ep-broad-mountain-aftyry8d.c-2.us-west-2.aws.neon.tech/neondb?sslmode=require"
      AI_SERVICE_URL: http://ai_service:5001
      NODE_ENV: production
      WARMUP_ON_START: "false"  # âœ… Disable warmups for CPU optimization
      MAX_CHUNK_SIZE: "1500"     # âœ… Reduced chunk size for faster processing
      MAX_CHUNKS: "8"            # âœ… Limit chunks for CPU optimization
      CHUNK_TIMEOUT: "1800000"     # âœ… Increased timeout to 30 minutes for large documents
    volumes:
      - ./Legal_docs:/app/Legal_docs:ro
    depends_on:
      - ai_service
    restart: unless-stopped
    networks:
      - legal-ai-network

  ai_service:
    build:
      context: ./ai_service
    container_name: legal-ai-service
    ports:
      - "5001:5001"
    environment:
      FLASK_ENV: production
      OLLAMA_HOST: host.docker.internal:11434
      WARMUP_ON_START: "false"  # âœ… Disable warmups for CPU optimization
      OLLAMA_NUM_PARALLEL: "2"  # âœ… Optimized parallel processing
      OLLAMA_CONTEXT_LENGTH: "1024"  # âœ… Reduced context length
      MAX_TOKENS_PER_REQUEST: "300"  # âœ… Limit tokens for faster responses
      DEFAULT_MODEL: "mistral:7b-instruct-q4_0"  # âœ… Use optimized model
      # ðŸš€ Additional Ollama speed optimizations
      OLLAMA_KEEP_ALIVE: "5m"  # Keep model in memory
      OLLAMA_GPU_LAYERS: "0"  # Force CPU-only for consistency
      OLLAMA_THREADS: "4"  # Optimize CPU threads
      OLLAMA_BATCH_SIZE: "512"  # Optimize batch processing
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    networks:
      - legal-ai-network

networks:
  legal-ai-network:
    driver: bridge
